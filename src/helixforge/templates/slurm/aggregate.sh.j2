#!/bin/bash
# HelixForge Result Aggregation Script
# Combines outputs from all chunk processing tasks.
#
# Usage:
#   bash aggregate.sh [--check-only]
#
# Options:
#   --check-only   Only check completion status, don't aggregate

set -euo pipefail

# ============================================================================
# Configuration
# ============================================================================

WORK_DIR="{{ work_dir }}"
OUTPUT_DIR="{{ output_dir }}"
CHUNK_PLAN="{{ chunk_plan_path }}"
EXPECTED_CHUNKS="{{ n_chunks }}"
CHECK_ONLY=false

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --check-only)
            CHECK_ONLY=true
            shift
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# ============================================================================
# Check Completion Status
# ============================================================================

echo "=========================================="
echo "HelixForge Result Aggregation"
echo "Work directory: ${WORK_DIR}"
echo "Output directory: ${OUTPUT_DIR}"
echo "Expected chunks: ${EXPECTED_CHUNKS}"
echo "=========================================="

# Count completed chunks
COMPLETED=$(ls -1 "${OUTPUT_DIR}"/*.done 2>/dev/null | wc -l || echo 0)
FAILED=$((EXPECTED_CHUNKS - COMPLETED))

echo ""
echo "Completion status:"
echo "  Expected: ${EXPECTED_CHUNKS}"
echo "  Completed: ${COMPLETED}"
echo "  Failed/Missing: ${FAILED}"

if [ "$FAILED" -gt 0 ]; then
    echo ""
    echo "Missing chunks:"
    for i in $(seq 0 $((EXPECTED_CHUNKS - 1))); do
        CHUNK_ID=$(printf "chunk_%04d" $i)
        if [ ! -f "${OUTPUT_DIR}/${CHUNK_ID}.done" ]; then
            echo "  - ${CHUNK_ID}"
        fi
    done
fi

if [ "$CHECK_ONLY" = true ]; then
    echo ""
    if [ "$FAILED" -eq 0 ]; then
        echo "All chunks completed successfully!"
        exit 0
    else
        echo "Some chunks failed. Check logs for details."
        exit 1
    fi
fi

if [ "$FAILED" -gt 0 ]; then
    echo ""
    echo "WARNING: ${FAILED} chunks did not complete. Proceeding with available results."
fi

# ============================================================================
# Aggregate GFF3 Files
# ============================================================================

{% if aggregate_gff %}
if ls "${OUTPUT_DIR}"/*.gff3 1> /dev/null 2>&1; then
    echo ""
    echo "Aggregating GFF3 files..."

    # Create combined GFF3 with proper header
    echo "##gff-version 3" > "${WORK_DIR}/{{ gff_output }}"
    echo "# Generated by HelixForge" >> "${WORK_DIR}/{{ gff_output }}"
    echo "# Source chunks: ${COMPLETED}" >> "${WORK_DIR}/{{ gff_output }}"
    echo "# Date: $(date -Iseconds)" >> "${WORK_DIR}/{{ gff_output }}"

    # Concatenate data lines (skip headers and comments)
    for f in $(ls "${OUTPUT_DIR}"/*.gff3 | sort); do
        grep -v "^#" "$f" 2>/dev/null || true
    done | sort -k1,1 -k4,4n >> "${WORK_DIR}/{{ gff_output }}"

    # Count features
    N_GENES=$(grep -c "\\tgene\\t" "${WORK_DIR}/{{ gff_output }}" 2>/dev/null || echo 0)
    N_MRNAS=$(grep -c "\\tmRNA\\t" "${WORK_DIR}/{{ gff_output }}" 2>/dev/null || echo 0)

    echo "  Combined GFF3: ${WORK_DIR}/{{ gff_output }}"
    echo "  Genes: ${N_GENES}"
    echo "  Transcripts: ${N_MRNAS}"
fi
{% endif %}

# ============================================================================
# Aggregate TSV Reports
# ============================================================================

{% if aggregate_tsv %}
if ls "${OUTPUT_DIR}"/*.tsv 1> /dev/null 2>&1; then
    echo ""
    echo "Aggregating TSV reports..."

    # Get first file to extract header
    FIRST_TSV=$(ls "${OUTPUT_DIR}"/*.tsv | head -1)

    # Write header from first file
    head -1 "$FIRST_TSV" > "${WORK_DIR}/{{ tsv_output }}"

    # Append data from all files (skip headers)
    for f in $(ls "${OUTPUT_DIR}"/*.tsv | sort); do
        tail -n +2 "$f" 2>/dev/null || true
    done >> "${WORK_DIR}/{{ tsv_output }}"

    N_ROWS=$(tail -n +2 "${WORK_DIR}/{{ tsv_output }}" | wc -l)
    echo "  Combined TSV: ${WORK_DIR}/{{ tsv_output }}"
    echo "  Data rows: ${N_ROWS}"
fi
{% endif %}

# ============================================================================
# Aggregate JSON Reports
# ============================================================================

{% if aggregate_json %}
if ls "${OUTPUT_DIR}"/*.json 1> /dev/null 2>&1; then
    echo ""
    echo "Aggregating JSON reports..."

    python3 -c "
import json
import glob

results = []
for f in sorted(glob.glob('${OUTPUT_DIR}/*.json')):
    if f.endswith('_plan.json'):
        continue
    with open(f) as fp:
        try:
            data = json.load(fp)
            if isinstance(data, list):
                results.extend(data)
            else:
                results.append(data)
        except json.JSONDecodeError:
            print(f'Warning: Could not parse {f}')

with open('${WORK_DIR}/{{ json_output }}', 'w') as fp:
    json.dump(results, fp, indent=2)
print(f'  Combined JSON: ${WORK_DIR}/{{ json_output }}')
print(f'  Items: {len(results)}')
"
fi
{% endif %}

# ============================================================================
# Generate Summary
# ============================================================================

echo ""
echo "=========================================="
echo "Aggregation complete"
echo "=========================================="

{% if generate_summary %}
# Generate summary report
python3 << 'PYTHON_SCRIPT'
import os
import json
from datetime import datetime

work_dir = "{{ work_dir }}"
output_dir = "{{ output_dir }}"
expected = {{ n_chunks }}

# Collect timing info from logs
log_dir = os.path.join(work_dir, "logs")
timings = []
if os.path.isdir(log_dir):
    for f in os.listdir(log_dir):
        if f.endswith(".out"):
            try:
                with open(os.path.join(log_dir, f)) as fp:
                    for line in fp:
                        if "Processing time:" in line:
                            t = float(line.split(":")[1].strip().split()[0])
                            timings.append(t)
            except:
                pass

# Count outputs
n_done = len([f for f in os.listdir(output_dir) if f.endswith(".done")])
n_gff = len([f for f in os.listdir(output_dir) if f.endswith(".gff3")])
n_tsv = len([f for f in os.listdir(output_dir) if f.endswith(".tsv")])

summary = {
    "date": datetime.now().isoformat(),
    "expected_chunks": expected,
    "completed_chunks": n_done,
    "gff_files": n_gff,
    "tsv_files": n_tsv,
}

if timings:
    summary["timing"] = {
        "mean_seconds": sum(timings) / len(timings),
        "max_seconds": max(timings),
        "min_seconds": min(timings),
        "total_seconds": sum(timings),
    }

with open(os.path.join(work_dir, "summary.json"), "w") as fp:
    json.dump(summary, fp, indent=2)

print(f"Summary written to {work_dir}/summary.json")
PYTHON_SCRIPT
{% endif %}

echo ""
echo "Output files:"
ls -lh "${WORK_DIR}"/*.gff3 2>/dev/null || true
ls -lh "${WORK_DIR}"/*.tsv 2>/dev/null || true
ls -lh "${WORK_DIR}"/*.json 2>/dev/null || true
